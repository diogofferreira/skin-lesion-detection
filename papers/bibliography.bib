
@online{noauthor_melanoma_nodate,
	title = {Melanoma - {SkinCancer}.org},
	url = {https://www.skincancer.org/skin-cancer-information/melanoma#panel1-5},
	titleaddon = {{SkinCancer}.org},
	urldate = {2018-12-31},
	file = {Melanoma - SkinCancer.org:/Users/pbm/Zotero/storage/8ILQG3IM/melanoma.html:text/html}
}

@inproceedings{ali_melanoma_2017,
	location = {Ras Al Khaimah},
	title = {Melanoma detection using regular convolutional neural networks},
	isbn = {978-1-5386-0872-2},
	url = {http://ieeexplore.ieee.org/document/8252041/},
	doi = {10.1109/ICECTA.2017.8252041},
	abstract = {In this paper, we propose a method for classifying melanoma images into benign and malignant using Convolutional Neural Networks ({CNNs}). Having an automated method for melanoma detection will assist dermatologists in the early diagnosis of this type of skin cancer. A regular convolutional network employing a modest number of parameters is used to detect melanoma images. The architecture is used to classify the dataset of the {ISBI} 2016 challenge in melanoma classiﬁcation. The dataset was not segmented or cropped prior to classiﬁcation. The proposed method was then evaluated for accuracy, sensitivity and speciﬁcity. Comparisons with the winning entry in the competition demonstrate that one can achieve a performance level comparable to state-of-the-art using standard convolutional neural network architectures that employ a lower number of parameters.},
	eventtitle = {2017 International Conference on Electrical and Computing Technologies and Applications ({ICECTA})},
	pages = {1--5},
	booktitle = {2017 International Conference on Electrical and Computing Technologies and Applications ({ICECTA})},
	publisher = {{IEEE}},
	author = {Ali, Aya Abu and Al-Marzouqi, Hasan},
	urldate = {2018-12-31},
	date = {2017-11},
	langid = {english},
	file = {Ali and Al-Marzouqi - 2017 - Melanoma detection using regular convolutional neu.pdf:/Users/pbm/Zotero/storage/5M7YTH9F/Ali and Al-Marzouqi - 2017 - Melanoma detection using regular convolutional neu.pdf:application/pdf}
}

@article{li_automatic_2014,
	title = {Automatic diagnosis of melanoma using machine learning methods on a spectroscopic system},
	volume = {14},
	issn = {1471-2342},
	url = {http://bmcmedimaging.biomedcentral.com/articles/10.1186/1471-2342-14-36},
	doi = {10.1186/1471-2342-14-36},
	number = {1},
	journaltitle = {{BMC} Medical Imaging},
	author = {Li, Lin and Zhang, Qizhi and Ding, Yihua and Jiang, Huabei and Thiers, Bruce H and Wang, James Z},
	urldate = {2018-12-31},
	date = {2014-12},
	langid = {english},
	file = {Full Text:/Users/pbm/Zotero/storage/IQN397KV/Li et al. - 2014 - Automatic diagnosis of melanoma using machine lear.pdf:application/pdf}
}

@article{haenssle_man_2018,
	title = {Man against machine: diagnostic performance of a deep learning convolutional neural network for dermoscopic melanoma recognition in comparison to 58 dermatologists},
	volume = {29},
	issn = {0923-7534, 1569-8041},
	url = {https://academic.oup.com/annonc/article/29/8/1836/5004443},
	doi = {10.1093/annonc/mdy166},
	shorttitle = {Man against machine},
	abstract = {Background: Deep learning convolutional neural networks ({CNN}) may facilitate melanoma detection, but data comparing a {CNN}’s diagnostic performance to larger groups of dermatologists are lacking.
Methods: Google’s Inception v4 {CNN} architecture was trained and validated using dermoscopic images and corresponding diagnoses. In a comparative cross-sectional reader study a 100-image test-set was used (level-I: dermoscopy only; level-{II}: dermoscopy plus clinical information and images). Main outcome measures were sensitivity, specificity and area under the curve ({AUC}) of receiver operating characteristics ({ROC}) for diagnostic classification (dichotomous) of lesions by the {CNN} versus an international group of 58 dermatologists during level-I or -{II} of the reader study. Secondary end points included the dermatologists’ diagnostic performance in their management decisions and differences in the diagnostic performance of dermatologists during level-I and -{II} of the reader study. Additionally, the {CNN}’s performance was compared with the top-five algorithms of the 2016 International Symposium on Biomedical Imaging ({ISBI}) challenge.
Results: In level-I dermatologists achieved a mean (6standard deviation) sensitivity and specificity for lesion classification of 86.6\% (69.3\%) and 71.3\% (611.2\%), respectively. More clinical information (level-{II}) improved the sensitivity to 88.9\% (69.6\%, P ¼ 0.19) and specificity to 75.7\% (611.7\%, P {\textless} 0.05). The {CNN} {ROC} curve revealed a higher specificity of 82.5\% when compared with dermatologists in level-I (71.3\%, P {\textless} 0.01) and level-{II} (75.7\%, P {\textless} 0.01) at their sensitivities of 86.6\% and 88.9\%, respectively. The {CNN} {ROC} {AUC} was greater than the mean {ROC} area of dermatologists (0.86 versus 0.79, P {\textless} 0.01). The {CNN} scored results close to the top three algorithms of the {ISBI} 2016 challenge.
Conclusions: For the first time we compared a {CNN}’s diagnostic performance with a large international group of 58 dermatologists, including 30 experts. Most dermatologists were outperformed by the {CNN}. Irrespective of any physicians’ experience, they may benefit from assistance by a {CNN}’s image classification. Clinical trial number: This study was registered at the German Clinical Trial Register ({DRKS}-Study-{ID}: {DRKS}00013570; https:// www.drks.de/drks\_web/).},
	pages = {1836--1842},
	number = {8},
	journaltitle = {Annals of Oncology},
	author = {Haenssle, H A and Fink, C and Schneiderbauer, R and Toberer, F and Buhl, T and Blum, A and Kalloo, A and Hassen, A Ben Hadj and Thomas, L and Enk, A and Uhlmann, L and {Reader study level-I and level-II Groups} and Alt, Christina and Arenbergerova, Monika and Bakos, Renato and Baltzer, Anne and Bertlich, Ines and Blum, Andreas and Bokor-Billmann, Therezia and Bowling, Jonathan and Braghiroli, Naira and Braun, Ralph and Buder-Bakhaya, Kristina and Buhl, Timo and Cabo, Horacio and Cabrijan, Leo and Cevic, Naciye and Classen, Anna and Deltgen, David and Fink, Christine and Georgieva, Ivelina and Hakim-Meibodi, Lara-Elena and Hanner, Susanne and Hartmann, Franziska and Hartmann, Julia and Haus, Georg and Hoxha, Elti and Karls, Raimonds and Koga, Hiroshi and Kreusch, Jürgen and Lallas, Aimilios and Majenka, Pawel and Marghoob, Ash and Massone, Cesare and Mekokishvili, Lali and Mestel, Dominik and Meyer, Volker and Neuberger, Anna and Nielsen, Kari and Oliviero, Margaret and Pampena, Riccardo and Paoli, John and Pawlik, Erika and Rao, Barbar and Rendon, Adriana and Russo, Teresa and Sadek, Ahmed and Samhaber, Kinga and Schneiderbauer, Roland and Schweizer, Anissa and Toberer, Ferdinand and Trennheuser, Lukas and Vlahova, Lyobomira and Wald, Alexander and Winkler, Julia and Wölbing, Priscila and Zalaudek, Iris},
	urldate = {2019-01-02},
	date = {2018-08-01},
	langid = {english},
	file = {Haenssle et al. - 2018 - Man against machine diagnostic performance of a d.pdf:/Users/pbm/Zotero/storage/A8VNGKBZ/Haenssle et al. - 2018 - Man against machine diagnostic performance of a d.pdf:application/pdf}
}

@inproceedings{nasr-esfahani_melanoma_2016,
	location = {Orlando, {FL}, {USA}},
	title = {Melanoma detection by analysis of clinical images using convolutional neural network},
	isbn = {978-1-4577-0220-4},
	url = {http://ieeexplore.ieee.org/document/7590963/},
	doi = {10.1109/EMBC.2016.7590963},
	abstract = {Melanoma, most threatening type of skin cancer, is on the rise. In this paper an implementation of a deeplearning system on a computer server, equipped with graphic processing unit ({GPU}), is proposed for detection of melanoma lesions. Clinical (non-dermoscopic) images are used in the proposed system, which could assist a dermatologist in early diagnosis of this type of skin cancer. In the proposed system, input clinical images, which could contain illumination and noise effects, are preprocessed in order to reduce such artifacts. Afterward, the enhanced images are fed to a pre-trained convolutional neural network ({CNN}) which is a member of deep learning models. The {CNN} classifier, which is trained by large number of training samples, distinguishes between melanoma and benign cases. Experimental results show that the proposed method is superior in terms of diagnostic accuracy in comparison with the state-of-the-art methods.},
	eventtitle = {2016 38th Annual International Conference of the {IEEE} Engineering in Medicine and Biology Society ({EMBC})},
	pages = {1373--1376},
	booktitle = {2016 38th Annual International Conference of the {IEEE} Engineering in Medicine and Biology Society ({EMBC})},
	publisher = {{IEEE}},
	author = {Nasr-Esfahani, E. and Samavi, S. and Karimi, N. and Soroushmehr, S.M.R. and Jafari, M.H. and Ward, K. and Najarian, K.},
	urldate = {2019-01-02},
	date = {2016-08},
	langid = {english},
	file = {Nasr-Esfahani et al. - 2016 - Melanoma detection by analysis of clinical images .pdf:/Users/pbm/Zotero/storage/5BGCN5LQ/Nasr-Esfahani et al. - 2016 - Melanoma detection by analysis of clinical images .pdf:application/pdf}
}

@inproceedings{dhungel_automated_2015,
	location = {Adelaide, Australia},
	title = {Automated Mass Detection in Mammograms Using Cascaded Deep Learning and Random Forests},
	isbn = {978-1-4673-6795-0},
	url = {http://ieeexplore.ieee.org/document/7371234/},
	doi = {10.1109/DICTA.2015.7371234},
	abstract = {Mass detection from mammograms plays a crucial role as a pre-processing stage for mass segmentation and classiﬁcation. The detection of masses from mammograms is considered to be a challenging problem due to their large variation in shape, size, boundary and texture and also because of their low signal to noise ratio compared to the surrounding breast tissue. In this paper, we present a novel approach for detecting masses in mammograms using a cascade of deep learning and random forest classiﬁers. The ﬁrst stage classiﬁer consists of a multi-scale deep belief network that selects suspicious regions to be further processed by a two-level cascade of deep convolutional neural networks. The regions that survive this deep learning analysis are then processed by a two-level cascade of random forest classiﬁers that use morphological and texture features extracted from regions selected along the cascade. Finally, regions that survive the cascade of random forest classiﬁers are combined using connected component analysis to produce state-of-the-art results. We also show that the proposed cascade of deep learning and random forest classiﬁers are effective in the reduction of false positive regions, while maintaining a high true positive detection rate. We tested our mass detection system on two publicly available datasets: {DDSM}-{BCRP} and {INbreast}. The ﬁnal mass detection produced by our approach achieves the best results on these publicly available datasets with a true positive rate of 0.96 ± 0.03 at 1.2 false positive per image on {INbreast} and true positive rate of 0.75 at 4.8 false positive per image on {DDSM}-{BCRP}.},
	eventtitle = {2015 International Conference on Digital Image Computing: Techniques and Applications ({DICTA})},
	pages = {1--8},
	booktitle = {2015 International Conference on Digital Image Computing: Techniques and Applications ({DICTA})},
	publisher = {{IEEE}},
	author = {Dhungel, Neeraj and Carneiro, Gustavo and Bradley, Andrew P.},
	urldate = {2019-01-02},
	date = {2015-11},
	langid = {english},
	file = {Dhungel et al. - 2015 - Automated Mass Detection in Mammograms Using Casca.pdf:/Users/pbm/Zotero/storage/GTCVDITI/Dhungel et al. - 2015 - Automated Mass Detection in Mammograms Using Casca.pdf:application/pdf}
}

@article{liu_early_nodate,
	title = {Early Diagnosis of Alzheimer's Disease with Deep Learning},
	abstract = {The accurate diagnosis of Alzheimer’s disease ({AD}) plays a significant role in patient care, especially at the early stage, because the consciousness of the severity and the progression risks allows the patients to take prevention measures before irreversible brain damages are shaped. Although many studies have applied machine learning methods for computer-aided-diagnosis ({CAD}) of {AD} recently, a bottleneck of the diagnosis performance was shown in most of the existing researches, mainly due to the congenital limitations of the chosen learning models. In this study, we design a deep learning architecture, which contains stacked auto-encoders and a softmax output layer, to overcome the bottleneck and aid the diagnosis of {AD} and its prodromal stage, Mild Cognitive Impairment ({MCI}). Compared to the previous workflows, our method is capable of analyzing multiple classes in one setting, and requires less labeled training samples and minimal domain prior knowledge. A significant performance gain on classification of all diagnosis groups was achieved in our experiments.},
	pages = {4},
	author = {Liu, Siqi* and Liu, Sidong and Cai, Weidong and Pujol, Sonia and Kikinis, Ron and Feng, Dagan},
	langid = {english},
	file = {Liu et al. - Early Diagnosis of Alzheimer's Disease with Deep L.pdf:/Users/pbm/Zotero/storage/XFL3U7RF/Liu et al. - Early Diagnosis of Alzheimer's Disease with Deep L.pdf:application/pdf}
}

@online{noauthor_skinvision_nodate,
	title = {{SkinVision} {\textbar} Skin Cancer Melanoma Detection App {\textbar} Check Your Skin},
	url = {https://www.skinvision.com/},
	urldate = {2019-01-02},
	file = {SkinVision | Skin Cancer Melanoma Detection App | Check Your Skin:/Users/pbm/Zotero/storage/KRRGXF3E/www.skinvision.com.html:text/html}
}

@online{noauthor_covalic_nodate,
	title = {Covalic},
	url = {https://challenge.kitware.com/#phase/5840f53ccad3a51cc66c8dab},
	titleaddon = {{ISIC} 2017 Challenge},
	urldate = {2019-01-06},
	file = {Covalic:/Users/pbm/Zotero/storage/ZM2HVTLL/challenge.kitware.com.html:text/html}
}

@inproceedings{ciresan_transfer_2012,
	title = {Transfer learning for Latin and Chinese characters with Deep Neural Networks},
	doi = {10.1109/IJCNN.2012.6252544},
	abstract = {We analyze transfer learning with Deep Neural Networks ({DNN}) on various character recognition tasks. {DNN} trained on digits are perfectly capable of recognizing uppercase letters with minimal retraining. They are on par with {DNN} fully trained on uppercase letters, but train much faster. {DNN} trained on Chinese characters easily recognize uppercase Latin letters. Learning Chinese characters is accelerated by first pretraining a {DNN} on a small subset of all classes and then continuing to train on all classes. Furthermore, pretrained nets consistently outperform randomly initialized nets on new tasks with few labeled data.},
	eventtitle = {The 2012 International Joint Conference on Neural Networks ({IJCNN})},
	pages = {1--6},
	booktitle = {The 2012 International Joint Conference on Neural Networks ({IJCNN})},
	author = {Cireşan, D. C. and Meier, U. and Schmidhuber, J.},
	date = {2012-06},
	keywords = {Artificial neural networks, character recognition, character recognition tasks, Chinese characters, deep neural networks, Error analysis, Feature extraction, Latin characters, learning (artificial intelligence), minimal retraining, natural language processing, neural nets, Neurons, {NIST}, pretrained nets, Training, transfer learning, uppercase letters},
	file = {IEEE Xplore Abstract Record:/Users/pbm/Zotero/storage/IESQIUU8/6252544.html:text/html;Submitted Version:/Users/pbm/Zotero/storage/LD2GVQNE/Cireşan et al. - 2012 - Transfer learning for Latin and Chinese characters.pdf:application/pdf}
}

@inproceedings{ng_deep_2015,
	location = {Seattle, Washington, {USA}},
	title = {Deep Learning for Emotion Recognition on Small Datasets using Transfer Learning},
	isbn = {978-1-4503-3912-4},
	url = {http://dl.acm.org/citation.cfm?doid=2818346.2830593},
	doi = {10.1145/2818346.2830593},
	abstract = {This paper presents the techniques employed in our team’s submissions to the 2015 Emotion Recognition in the Wild contest, for the sub-challenge of Static Facial Expression Recognition in the Wild. The objective of this sub-challenge is to classify the emotions expressed by the primary human subject in static images extracted from movies. We follow a transfer learning approach for deep Convolutional Neural Network ({CNN}) architectures. Starting from a network pre-trained on the generic {ImageNet} dataset, we perform supervised ﬁne-tuning on the network in a two-stage process, ﬁrst on datasets relevant to facial expressions, followed by the contest’s dataset. Experimental results show that this cascading ﬁne-tuning approach achieves better results, compared to a single stage ﬁnetuning with the combined datasets. Our best submission exhibited an overall accuracy of 48.5\% in the validation set and 55.6\% in the test set, which compares favorably to the respective 35.96\% and 39.13\% of the challenge baseline.},
	eventtitle = {the 2015 {ACM}},
	pages = {443--449},
	booktitle = {Proceedings of the 2015 {ACM} on International Conference on Multimodal Interaction - {ICMI} '15},
	publisher = {{ACM} Press},
	author = {Ng, Hong-Wei and Nguyen, Viet Dung and Vonikakis, Vassilios and Winkler, Stefan},
	urldate = {2019-01-06},
	date = {2015},
	langid = {english},
	file = {Ng et al. - 2015 - Deep Learning for Emotion Recognition on Small Dat.pdf:/Users/pbm/Zotero/storage/CU7IZPM9/Ng et al. - 2015 - Deep Learning for Emotion Recognition on Small Dat.pdf:application/pdf}
}

@article{hu_transfer_2016,
	title = {Transfer learning for short-term wind speed prediction with deep neural networks},
	volume = {85},
	issn = {0960-1481},
	url = {http://www.sciencedirect.com/science/article/pii/S0960148115300574},
	doi = {10.1016/j.renene.2015.06.034},
	abstract = {As a type of clean and renewable energy source, wind power is widely used. However, owing to the uncertainty of wind speed, it is essential to build an accurate forecasting model for large-scale wind power penetration. Numerical weather prediction ({NWP}) and data-driven modeling are two typical paradigms. {NWP} is usually unavailable or spatially insufficient. Data-driven modeling is an effective candidate. As to some newly-built wind farms, sufficient historical data is not available for training an accurate model, while some older wind farms may have long-term wind speed records. A question arises regarding whether the prediction model trained by data coming from older farms is also effective for a newly-built farm. In this paper, we propose an interesting trial of transferring the information obtained from data-rich farms to a newly-built farm. It is well known that deep learning can extract a high-level representation of raw data. We introduce deep neural networks, trained by data from data-rich farms, to extract wind speed patterns, and then finely tune the mapping with data coming from newly-built farms. In this way, the trained network transfers information from one farm to another. The experimental results show that prediction errors are significantly reduced using the proposed technique.},
	pages = {83--95},
	journaltitle = {Renewable Energy},
	shortjournal = {Renewable Energy},
	author = {Hu, Qinghua and Zhang, Rujia and Zhou, Yucan},
	urldate = {2019-01-06},
	date = {2016-01-01},
	keywords = {Deep neural networks, Stacked denoising autoencoder, Transfer learning, Wind speed prediction},
	file = {ScienceDirect Snapshot:/Users/pbm/Zotero/storage/GQRE4N7H/S0960148115300574.html:text/html}
}

@online{noauthor_imagenet_nodate,
	title = {{ImageNet}},
	url = {http://www.image-net.org/},
	titleaddon = {{ImageNet}},
	urldate = {2019-01-06},
	file = {ImageNet:/Users/pbm/Zotero/storage/JGETX62X/www.image-net.org.html:text/html}
}

@article{simonyan_very_2014,
	title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
	url = {http://arxiv.org/abs/1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our {ImageNet} Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing {ConvNet} models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	journaltitle = {{arXiv}:1409.1556 [cs]},
	author = {Simonyan, Karen and Zisserman, Andrew},
	urldate = {2019-01-06},
	date = {2014-09-04},
	eprinttype = {arxiv},
	eprint = {1409.1556},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1409.1556 PDF:/Users/pbm/Zotero/storage/4GUM4WZ2/Simonyan and Zisserman - 2014 - Very Deep Convolutional Networks for Large-Scale I.pdf:application/pdf;arXiv.org Snapshot:/Users/pbm/Zotero/storage/B4LB8BX2/1409.html:text/html}
}

@article{he_deep_2015,
	title = {Deep Residual Learning for Image Recognition},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the {ImageNet} dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than {VGG} nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the {ImageNet} test set. This result won the 1st place on the {ILSVRC} 2015 classification task. We also present analysis on {CIFAR}-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the {COCO} object detection dataset. Deep residual nets are foundations of our submissions to {ILSVRC} \& {COCO} 2015 competitions, where we also won the 1st places on the tasks of {ImageNet} detection, {ImageNet} localization, {COCO} detection, and {COCO} segmentation.},
	journaltitle = {{arXiv}:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	urldate = {2019-01-06},
	date = {2015-12-10},
	eprinttype = {arxiv},
	eprint = {1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1512.03385 PDF:/Users/pbm/Zotero/storage/9M292RBA/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv.org Snapshot:/Users/pbm/Zotero/storage/348K8IRY/1512.html:text/html}
}

@article{matsunaga_image_2017,
	title = {Image Classification of Melanoma, Nevus and Seborrheic Keratosis by Deep Neural Network Ensemble},
	url = {http://arxiv.org/abs/1703.03108},
	abstract = {This short paper reports the method and the evaluation results of Casio and Shinshu University joint team for the {ISBI} Challenge 2017 - Skin Lesion Analysis Towards Melanoma Detection - Part 3: Lesion Classification hosted by {ISIC}. Our online validation score was 0.958 with melanoma classifier {AUC} 0.924 and seborrheic keratosis classifier {AUC} 0.993.},
	journaltitle = {{arXiv}:1703.03108 [cs]},
	author = {Matsunaga, Kazuhisa and Hamada, Akira and Minagawa, Akane and Koga, Hiroshi},
	urldate = {2019-01-07},
	date = {2017-03-08},
	eprinttype = {arxiv},
	eprint = {1703.03108},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1703.03108 PDF:/Users/pbm/Zotero/storage/22E8FPX9/Matsunaga et al. - 2017 - Image Classification of Melanoma, Nevus and Seborr.pdf:application/pdf;arXiv.org Snapshot:/Users/pbm/Zotero/storage/MEUSINAC/1703.html:text/html}
}

@online{johnson_nist_2010,
	title = {{NIST} Special Database 19},
	url = {https://www.nist.gov/srd/nist-special-database-19},
	abstract = {{NIST} Handprinted Forms and Characters Database Special Database 19 contains {NIST}'s entire corpus of training materials...},
	titleaddon = {{NIST}},
	author = {Johnson, Sherena G.},
	urldate = {2019-01-08},
	date = {2010-08-27},
	langid = {english}
}

@incollection{krizhevsky_imagenet_2012,
	title = {{ImageNet} Classification with Deep Convolutional Neural Networks},
	url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
	pages = {1097--1105},
	booktitle = {Advances in Neural Information Processing Systems 25},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
	urldate = {2019-01-08},
	date = {2012},
	file = {NIPS Full Text PDF:/Users/pbm/Zotero/storage/KJ4BYJRF/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf:application/pdf;NIPS Snapshot:/Users/pbm/Zotero/storage/VDK4UDSV/4824-imagenet-classification-with-deep-convolutional-neural-networks.html:text/html}
}

@article{chatfield_return_2014,
	title = {Return of the Devil in the Details: Delving Deep into Convolutional Nets},
	url = {http://arxiv.org/abs/1405.3531},
	shorttitle = {Return of the Devil in the Details},
	abstract = {The latest generation of Convolutional Neural Networks ({CNN}) have achieved impressive results in challenging benchmarks on image recognition and object detection, significantly raising the interest of the community in these methods. Nevertheless, it is still unclear how different {CNN} methods compare with each other and with previous state-of-the-art shallow representations such as the Bag-of-Visual-Words and the Improved Fisher Vector. This paper conducts a rigorous evaluation of these new techniques, exploring different deep architectures and comparing them on a common ground, identifying and disclosing important implementation details. We identify several useful properties of {CNN}-based representations, including the fact that the dimensionality of the {CNN} output layer can be reduced significantly without having an adverse effect on performance. We also identify aspects of deep and shallow methods that can be successfully shared. In particular, we show that the data augmentation techniques commonly applied to {CNN}-based methods can also be applied to shallow methods, and result in an analogous performance boost. Source code and models to reproduce the experiments in the paper is made publicly available.},
	journaltitle = {{arXiv}:1405.3531 [cs]},
	author = {Chatfield, Ken and Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
	urldate = {2019-01-08},
	date = {2014-05-14},
	eprinttype = {arxiv},
	eprint = {1405.3531},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1405.3531 PDF:/Users/pbm/Zotero/storage/EF87M9HK/Chatfield et al. - 2014 - Return of the Devil in the Details Delving Deep i.pdf:application/pdf;arXiv.org Snapshot:/Users/pbm/Zotero/storage/KS92K76J/1405.html:text/html}
}